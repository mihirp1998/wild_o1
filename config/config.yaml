# Training and evaluation arguments
causal_llm: true
learning_rate: 2.0e-4
num_train_epochs: 500
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
gradient_checkpointing: false
logging_steps: 25
save_steps: 2500
packing: false
model_name: "meta-llama/Llama-3.1-8B-Instruct"
output_dir: "${CKPT_DIR}"
load_dir: null
eval_strategy: "steps"
push_to_hub: false
rand_train: false
use_n_shot_prompt: 0
max_new_tokens: 400
num_samples: 20
generate_every_n_steps: 500
max_seq_length: 1024
weight_decay: 0.0
prompt_version: 1
perplexity_device: 1
debug_with_single_example: false
eval_steps: 500

# PEFT arguments
use_incontext: false
use_peft: false
lora_r: 32
lora_alpha: 16
